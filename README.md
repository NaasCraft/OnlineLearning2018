# Online Learning project - 2018

ENSAE project for the class "Online Learning and Aggregation", April 2018

Authors: Samuel Ritchie, Guillaume Demonet

+ _Abstract_:

  In this project, we analyze the models presented in [1] as solutions to a Bayesian model of the classical Multi-Armed Bandit Problem. We provide a concise analysis of the algorithms' theroretical regret and expected success rate. We also compare empirical results of these models with algorithms suggested in [2] (TBD).

  _[1]: [Multi-armed Bandit Models for the
Optimal Design of Clinical Trials: Benefits
and Challenges](https://arxiv.org/pdf/1507.08025.pdf),  
  Sof√≠a S. Villar, Jack Bowden and James Wason, 2015_

## Theoretical study

[TODO]

_Goals_:
- [ ] General formulation of the problem
  - [ ] Simple results for basic algorithms (no Bayesian approach) ?
- [ ] Theoretical regret analysis (bounds ?) using convex optimization
- [ ] Comparison with results exposed by the article


## Empirical study

[TODO]

- Construct benchmarking framework
  - Multi-armed environment (population) generation (Bernouilli, Gaussian)
  - Reporting system (performance, regret, etc.)
- Algorithms API (TBD)
- Reinforcement Learning (guarantees ?) algorithm TBD